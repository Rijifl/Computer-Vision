{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac22af2",
   "metadata": {},
   "source": [
    "# Model Validation on Holdout Set\n",
    "\n",
    "This notebook evaluates a trained ResNet model on a holdout dataset that wasn't used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8feb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52abfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Settings and Configuration\n",
    "OUTPUT_FOLDER = \"output_balanced\"  # Where your model is saved\n",
    "HOLDOUT_FOLDER = \"path/to/your/holdout/images\"  # CHANGE THIS to your holdout folder\n",
    "IMAGE_SIZE = 224\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"Holdout folder: {HOLDOUT_FOLDER}\")\n",
    "print(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16829ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Utility Functions\n",
    "def get_numeric_label(filename):\n",
    "    \"\"\"Extract numeric value from filename for regression\"\"\"\n",
    "    match = re.search(r'(\\d+)[pP]', filename)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "def make_square(image, size):\n",
    "    \"\"\"Resize image and pad to square while keeping RGB\"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    w, h = image.size\n",
    "    scale = size / max(w, h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    \n",
    "    resized = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    square = Image.new('RGB', (size, size), (255, 255, 255))\n",
    "    \n",
    "    x = (size - new_w) // 2\n",
    "    y = (size - new_h) // 2\n",
    "    square.paste(resized, (x, y))\n",
    "    \n",
    "    return square\n",
    "\n",
    "def round_to_interval(values, interval=5):\n",
    "    \"\"\"Round predictions to nearest interval (e.g., 5)\"\"\"\n",
    "    return np.round(np.array(values) / interval) * interval\n",
    "\n",
    "print(\"Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Image Processing Function\n",
    "def process_single_image(image_path, filename):\n",
    "    \"\"\"Process a single image and return tensor, label, filename\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path)\n",
    "    image = make_square(image, IMAGE_SIZE)\n",
    "    \n",
    "    # Apply ImageNet normalization\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    tensor = normalize(image)\n",
    "    \n",
    "    # Get label from filename\n",
    "    label = get_numeric_label(filename)\n",
    "    \n",
    "    return tensor, label, filename\n",
    "\n",
    "print(\"Image processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728da60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Trained Model\n",
    "print(\"Loading trained model...\")\n",
    "\n",
    "# Create model architecture (must match training setup)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(model.fc.in_features, 1)\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "model_path = f\"{OUTPUT_FOLDER}/trained_model.pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file not found at {model_path}\")\n",
    "    print(\"Please check the OUTPUT_FOLDER path and ensure the model was trained and saved.\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(f\"✓ Model loaded successfully from {model_path}\")\n",
    "    print(f\"✓ Model moved to {DEVICE}\")\n",
    "    print(f\"✓ Model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Scan Holdout Dataset\n",
    "print(f\"Scanning holdout folder: {HOLDOUT_FOLDER}\")\n",
    "\n",
    "if not os.path.exists(HOLDOUT_FOLDER):\n",
    "    print(f\"❌ Error: Holdout folder does not exist: {HOLDOUT_FOLDER}\")\n",
    "    print(\"Please update the HOLDOUT_FOLDER path in the settings cell.\")\n",
    "else:\n",
    "    # Get all image files from holdout folder\n",
    "    image_files = [f for f in os.listdir(HOLDOUT_FOLDER) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp'))]\n",
    "    \n",
    "    print(f\"✓ Found {len(image_files)} images in holdout set\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"⚠️ No images found! Please check the HOLDOUT_FOLDER path.\")\n",
    "    else:\n",
    "        # Show first few filenames as examples\n",
    "        print(\"Sample filenames:\")\n",
    "        for i, filename in enumerate(image_files[:5]):\n",
    "            label = get_numeric_label(filename)\n",
    "            print(f\"  {i+1}. {filename} (label: {label})\")\n",
    "        \n",
    "        if len(image_files) > 5:\n",
    "            print(f\"  ... and {len(image_files) - 5} more files\")\n",
    "        \n",
    "        print(f\"✓ Ready to evaluate on {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Make Predictions on Holdout Set\n",
    "print(\"Processing images and making predictions...\")\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_filenames = []\n",
    "\n",
    "# Process images in batches for memory efficiency\n",
    "for i in tqdm(range(0, len(image_files), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_files = image_files[i:i+BATCH_SIZE]\n",
    "    \n",
    "    # Process current batch\n",
    "    batch_tensors = []\n",
    "    batch_labels = []\n",
    "    batch_names = []\n",
    "    \n",
    "    for filename in batch_files:\n",
    "        image_path = os.path.join(HOLDOUT_FOLDER, filename)\n",
    "        try:\n",
    "            tensor, label, fname = process_single_image(image_path, filename)\n",
    "            batch_tensors.append(tensor)\n",
    "            batch_labels.append(label)\n",
    "            batch_names.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Skip empty batches\n",
    "    if len(batch_tensors) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Stack tensors into batch\n",
    "    batch_data = torch.stack(batch_tensors).to(DEVICE)\n",
    "    batch_targets = torch.tensor(batch_labels, dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    # Make predictions for this batch\n",
    "    with torch.no_grad():\n",
    "        batch_predictions = model(batch_data).view(-1)\n",
    "        batch_targets = batch_targets.view(-1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(batch_predictions.cpu().numpy())\n",
    "        all_targets.extend(batch_targets.cpu().numpy())\n",
    "        all_filenames.extend(batch_names)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(all_predictions)\n",
    "targets = np.array(all_targets)\n",
    "predictions_rounded = round_to_interval(predictions, 5)\n",
    "\n",
    "print(f\"✓ Successfully processed {len(predictions)} images\")\n",
    "print(f\"✓ Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee60bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Calculate Comprehensive Metrics\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"HOLDOUT SET EVALUATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset size: {len(predictions)} images\")\n",
    "\n",
    "# Basic metrics\n",
    "mse = mean_squared_error(targets, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(targets, predictions)\n",
    "r2 = r2_score(targets, predictions)\n",
    "\n",
    "# Rounded prediction metrics\n",
    "mse_rounded = mean_squared_error(targets, predictions_rounded)\n",
    "rmse_rounded = np.sqrt(mse_rounded)\n",
    "mae_rounded = mean_absolute_error(targets, predictions_rounded)\n",
    "r2_rounded = r2_score(targets, predictions_rounded)\n",
    "\n",
    "print(f\"\\nRAW PREDICTIONS:\")\n",
    "print(f\"  MSE:  {mse:.3f}\")\n",
    "print(f\"  RMSE: {rmse:.3f}\")\n",
    "print(f\"  MAE:  {mae:.3f}\")\n",
    "print(f\"  R²:   {r2:.3f}\")\n",
    "\n",
    "print(f\"\\nROUNDED PREDICTIONS (to nearest 5):\")\n",
    "print(f\"  MSE:  {mse_rounded:.3f}\")\n",
    "print(f\"  RMSE: {rmse_rounded:.3f}\")\n",
    "print(f\"  MAE:  {mae_rounded:.3f}\")\n",
    "print(f\"  R²:   {r2_rounded:.3f}\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Detailed Error Analysis\n",
    "# Error analysis\n",
    "abs_errors = np.abs(predictions_rounded - targets)\n",
    "pct_errors = np.where(targets != 0, abs_errors / targets * 100, abs_errors)\n",
    "\n",
    "perfect_predictions = np.sum(abs_errors == 0)\n",
    "within_5 = np.sum(abs_errors <= 5)\n",
    "within_10 = np.sum(abs_errors <= 10)\n",
    "\n",
    "print(f\"ERROR ANALYSIS:\")\n",
    "print(f\"  Perfect predictions: {perfect_predictions}/{len(targets)} ({perfect_predictions/len(targets)*100:.1f}%)\")\n",
    "print(f\"  Within ±5:          {within_5}/{len(targets)} ({within_5/len(targets)*100:.1f}%)\")\n",
    "print(f\"  Within ±10:         {within_10}/{len(targets)} ({within_10/len(targets)*100:.1f}%)\")\n",
    "print(f\"  Mean error:         {np.mean(pct_errors):.1f}%\")\n",
    "print(f\"  Median error:       {np.median(pct_errors):.1f}%\")\n",
    "print(f\"  Max error:          {np.max(abs_errors):.1f} ({np.max(pct_errors):.1f}%)\")\n",
    "\n",
    "# Per-class analysis\n",
    "unique_targets = sorted(set(targets))\n",
    "print(f\"\\nPER-CLASS ANALYSIS:\")\n",
    "for target_val in unique_targets:\n",
    "    mask = targets == target_val\n",
    "    if np.sum(mask) > 0:\n",
    "        class_mae = mean_absolute_error(targets[mask], predictions_rounded[mask])\n",
    "        class_count = np.sum(mask)\n",
    "        class_perfect = np.sum(abs_errors[mask] == 0)\n",
    "        print(f\"  {target_val:3.0f}p: {class_count:2d} samples, MAE: {class_mae:5.2f}, Perfect: {class_perfect:2d}/{class_count} ({class_perfect/class_count*100:4.1f}%)\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Show Worst Predictions\n",
    "print(f\"WORST PREDICTIONS (Top 10):\")\n",
    "error_indices = np.argsort(abs_errors)[::-1]\n",
    "\n",
    "for i in range(min(10, len(error_indices))):\n",
    "    idx = error_indices[i]\n",
    "    filename = all_filenames[idx]\n",
    "    print(f\"  {filename:35s} | Actual: {targets[idx]:5.1f} | Predicted: {predictions_rounded[idx]:5.1f} | Error: {abs_errors[idx]:5.1f}\")\n",
    "\n",
    "# Show some good predictions too\n",
    "print(f\"\\nBEST PREDICTIONS (Perfect matches):\")\n",
    "perfect_indices = np.where(abs_errors == 0)[0]\n",
    "if len(perfect_indices) > 0:\n",
    "    for i in range(min(10, len(perfect_indices))):\n",
    "        idx = perfect_indices[i]\n",
    "        filename = all_filenames[idx]\n",
    "        print(f\"  {filename:35s} | Actual: {targets[idx]:5.1f} | Predicted: {predictions_rounded[idx]:5.1f} | Error: {abs_errors[idx]:5.1f}\")\n",
    "else:\n",
    "    print(\"  No perfect predictions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Comprehensive Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Predictions vs Actual\n",
    "axes[0,0].scatter(targets, predictions_rounded, alpha=0.6, s=50)\n",
    "min_val, max_val = min(targets), max(targets)\n",
    "axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0,0].set_xlabel('Actual Values')\n",
    "axes[0,0].set_ylabel('Predicted Values')\n",
    "axes[0,0].set_title(f'Predictions vs Actual (R² = {r2_rounded:.3f})')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Error distribution\n",
    "axes[0,1].hist(pct_errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,1].set_xlabel('Percentage Error (%)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title(f'Error Distribution (Mean: {np.mean(pct_errors):.1f}%)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Absolute errors\n",
    "axes[1,0].hist(abs_errors, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1,0].set_xlabel('Absolute Error')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title(f'Absolute Error Distribution (MAE: {mae_rounded:.2f})')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Per-class MAE\n",
    "class_maes = []\n",
    "class_labels = []\n",
    "for target_val in unique_targets:\n",
    "    mask = targets == target_val\n",
    "    if np.sum(mask) > 0:\n",
    "        class_mae = mean_absolute_error(targets[mask], predictions_rounded[mask])\n",
    "        class_maes.append(class_mae)\n",
    "        class_labels.append(f'{target_val:.0f}p')\n",
    "\n",
    "bars = axes[1,1].bar(class_labels, class_maes, alpha=0.7, color='lightgreen')\n",
    "axes[1,1].set_xlabel('Class')\n",
    "axes[1,1].set_ylabel('Mean Absolute Error')\n",
    "axes[1,1].set_title('MAE by Class')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for bar, val in zip(bars, class_maes):\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                   f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"VALIDATION COMPLETE - FINAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✓ Model performance on {len(predictions)} holdout images:\")\n",
    "print(f\"  • R² Score: {r2_rounded:.3f}\")\n",
    "print(f\"  • RMSE: {rmse_rounded:.2f}\")\n",
    "print(f\"  • MAE: {mae_rounded:.2f}\")\n",
    "print(f\"  • Perfect predictions: {perfect_predictions/len(targets)*100:.1f}%\")\n",
    "print(f\"  • Within ±5 error: {within_5/len(targets)*100:.1f}%\")\n",
    "print(f\"  • Within ±10 error: {within_10/len(targets)*100:.1f}%\")\n",
    "\n",
    "# Model performance assessment\n",
    "if r2_rounded > 0.9:\n",
    "    performance = \"Excellent\"\n",
    "elif r2_rounded > 0.8:\n",
    "    performance = \"Very Good\"\n",
    "elif r2_rounded > 0.7:\n",
    "    performance = \"Good\"\n",
    "elif r2_rounded > 0.5:\n",
    "    performance = \"Fair\"\n",
    "else:\n",
    "    performance = \"Needs Improvement\"\n",
    "\n",
    "print(f\"\\n📊 Overall Performance: {performance}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'dataset_size': len(predictions),\n",
    "    'r2_score': float(r2_rounded),\n",
    "    'rmse': float(rmse_rounded),\n",
    "    'mae': float(mae_rounded),\n",
    "    'perfect_predictions_pct': float(perfect_predictions/len(targets)*100),\n",
    "    'within_5_pct': float(within_5/len(targets)*100),\n",
    "    'within_10_pct': float(within_10/len(targets)*100),\n",
    "    'performance_rating': performance\n",
    "}\n",
    "\n",
    "print(f\"Results summary saved to memory. Consider saving detailed results to a file if needed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
